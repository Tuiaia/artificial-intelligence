{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1S7ZaenxvS0TrDEau6zWA36CM1QPbVRZl","authorship_tag":"ABX9TyPnb3kNqED0zp0W2UT4jvx+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"lVIJ0bptUDY3"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"P6STAP2bnl9Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683410413647,"user_tz":240,"elapsed":27362,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}},"outputId":"484d649b-b184-4510-c47b-dcebd8d4da93"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers transformers_interpret"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPNOuUXTMpwb","executionInfo":{"status":"ok","timestamp":1683410427836,"user_tz":240,"elapsed":14457,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}},"outputId":"119bd172-edcb-476e-c232-4dfbabaf7916"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn.functional as F\n","from os import chdir\n","import pandas as pd"],"metadata":{"id":"stkG-OJ8M40o","executionInfo":{"status":"ok","timestamp":1683410435887,"user_tz":240,"elapsed":8064,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["REPO_DIR = '/content/drive/MyDrive/pantanal.dev/artificial-intelligence'\n","chdir(REPO_DIR)"],"metadata":{"id":"qZR0pwJkKiQY","executionInfo":{"status":"ok","timestamp":1683410436296,"user_tz":240,"elapsed":453,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["seed = 42\n","np.random.seed(seed)"],"metadata":{"id":"neatenCrUFW3","executionInfo":{"status":"ok","timestamp":1683410436298,"user_tz":240,"elapsed":7,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv('datasets/train_df.csv', sep='|')\n","val_df = pd.read_csv('datasets/val_df.csv', sep='|')\n","test_df = pd.read_csv('datasets/test_df.csv', sep='|')"],"metadata":{"id":"53X1dhf9Qs3A","executionInfo":{"status":"ok","timestamp":1683410438602,"user_tz":240,"elapsed":2310,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Carregar o tokenizer e o modelo BERT\n","model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/pantanal.dev/artificial-intelligence/trainings/bert-base-multilingual-cased-06/pruned/', output_attentions=True)\n","tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/pantanal.dev/artificial-intelligence/trainings/bert-base-multilingual-cased-06/pruned')"],"metadata":{"id":"ALGNfIL7NYsH","executionInfo":{"status":"ok","timestamp":1683412936677,"user_tz":240,"elapsed":15004,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["# Classification result insights"],"metadata":{"id":"RPdFJFTUI5jz"}},{"cell_type":"code","source":["!pip install langdetect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dK4JxON2FKVb","executionInfo":{"status":"ok","timestamp":1683410476483,"user_tz":240,"elapsed":12501,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}},"outputId":"d23aea0c-f019-40de-c293-ce0ea8d84ff2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=8b0fe920126dbf29962b6453225f6ce45cd5a4a83282c0b50e37b0338d5747b9\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}]},{"cell_type":"code","source":["if language == 'auto':\n","        language = detect(text)\n","        if language == 'en':\n","            language = 'english'\n","        else:\n","            language = 'portuguese'"],"metadata":{"id":"Xjt5LrGKFPyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from langdetect import detect\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Função para remover stopwords de um texto\n","def remove_stopwords(text: str, language: str = \"auto\") -> str:\n","    \"\"\"\n","    Remove stopwords de um texto em português ou inglês.\n","\n","    Args:\n","        text (str): O texto para remover as stopwords.\n","        language (str, optional): O idioma das stopwords. Por padrão, é utilizado \"auto\" para detectar o idioma automaticamente.\n","\n","    Returns:\n","        str: O texto sem stopwords.\n","\n","    Raises:\n","        ValueError: Se o idioma detectado automaticamente não for suportado pela aplicação.\n","\n","    Exemplo de uso:\n","        >>> texto = \"Este é um exemplo de texto em português que será processado para remoção de stopwords.\"\n","        >>> texto_sem_stopwords = remove_stopwords(texto, language=\"portuguese\")\n","        >>> print(texto_sem_stopwords)\n","        \"exemplo texto português processado remoção stopwords.\"\n","    \"\"\"\n","    if language == 'auto':\n","        language = detect(text)\n","        if language == 'en':\n","            language = 'english'\n","        else:\n","            language = 'portuguese'\n","\n","    words = nltk.word_tokenize(text)\n","    stopwords_list = set(stopwords.words(language))\n","    filtered_words = [word for word in words if word.lower() not in stopwords_list]\n","    return \" \".join(filtered_words)"],"metadata":{"id":"tlOI6jyOYhRV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683410477257,"user_tz":240,"elapsed":804,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}},"outputId":"48a2bbcf-3449-46d6-8194-580efa1a096e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["from string import punctuation\n","\n","def merge_subtokens(tokenizer, word_attributions: list[tuple]) -> list[tuple]:\n","    \"\"\"\n","    Agrupa as pontuações de tokens divididos do BERT em um único token.\n","    \n","    Args:\n","        tokenizer: Tokenizer BERT para usar.\n","        word_attributions (List[tuple]): As atribuições de palavras do tokenizador.\n","\n","    Returns:\n","        List[tuple]: Atribuições de palavras com tokens divididos mesclados.\n","    \"\"\"\n","    merged_attributions = []\n","    merged_token = ''\n","    merged_value = 0.0\n","\n","    for token, value in word_attributions:\n","        detokenized_token = tokenizer.convert_tokens_to_string([token]).strip()\n","        if detokenized_token:\n","            if detokenized_token in punctuation or detokenized_token in {'[CLS]', '[SEP]', '[UNK]'}:\n","                continue\n","            if token.startswith('##'):\n","                merged_token += token[2:]\n","                merged_value += value\n","            else:\n","                if merged_token:\n","                    merged_attributions.append((merged_token, merged_value))\n","                    merged_token = ''\n","                    merged_value = 0.0\n","                merged_token = detokenized_token\n","                merged_value = value\n","        else:\n","            merged_token += token.replace('##', '')\n","            merged_value += value\n","\n","    if merged_token:\n","        merged_attributions.append((merged_token, merged_value))\n","\n","    return merged_attributions"],"metadata":{"id":"6LKEtZ3dX8TU","executionInfo":{"status":"ok","timestamp":1683410481383,"user_tz":240,"elapsed":18,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def format_attributions(word_attributions: list[tuple[str, float]]) -> list[tuple[str, str]]:\n","    \"\"\"\n","    Formata as atribuições de palavras para uma representação mais intuitiva.\n","\n","    Args:\n","        word_attributions: Uma lista de tuplas contendo as atribuições de palavras. Cada tupla contém uma palavra/token\n","            e um valor float representando sua importância.\n","\n","    Returns:\n","        Uma lista de tuplas contendo as palavras formatadas e suas atribuições em formato de porcentagem, ordenadas por\n","        importância.\n","    \"\"\"\n","    # Obter o valor total das atribuições\n","    total = sum(abs(score) for _, score in word_attributions)\n","\n","    # Formatar cada atribuição como uma tupla (token, porcentagem)\n","    formatted_attributions = []\n","    for token, score in word_attributions:\n","        # Calcular a porcentagem e arredondar para duas casas decimais\n","        percentage = round((abs(score) / total) * 100, 2)\n","        formatted_attributions.append((token, percentage))\n","\n","    # Ordenar as atribuições por porcentagem descendente\n","    formatted_attributions = sorted(formatted_attributions, key=lambda x: x[1], reverse=True)\n","\n","    return formatted_attributions"],"metadata":{"id":"ofMknXt4pjNg","executionInfo":{"status":"ok","timestamp":1683410482435,"user_tz":240,"elapsed":23,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from transformers_interpret import SequenceClassificationExplainer\n","cls_explainer = SequenceClassificationExplainer(model, tokenizer)"],"metadata":{"id":"Lt0T_bsgZ5-4","executionInfo":{"status":"ok","timestamp":1683412936679,"user_tz":240,"elapsed":30,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["%timeit SequenceClassificationExplainer(model, tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_U2Ac2T5E-Zt","executionInfo":{"status":"ok","timestamp":1683412952633,"user_tz":240,"elapsed":15979,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}},"outputId":"02231a4c-c70a-4253-d1ce-ebdba64ca306"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["202 µs ± 32.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"]}]},{"cell_type":"code","source":["def classify_text(input_text):\n","    input_text = remove_stopwords(input_text)\n","    tokenized_input = tokenizer(input_text, truncation=True, max_length=100, return_tensors='pt')\n","    truncated_input_text = tokenizer.decode(tokenized_input['input_ids'][0])\n","    word_attributions = cls_explainer(truncated_input_text)\n","    word_attributions = merge_subtokens(tokenizer, word_attributions)\n","    word_attributions = sorted(word_attributions, key=lambda x: (-x[1], x[0]))\n","    word_attributions = format_attributions(word_attributions)\n","\n","    return {\n","        'predicition_class_name': cls_explainer.predicted_class_name,\n","        'prediction_index': cls_explainer.predicted_class_index,\n","        'prediction_probatility': cls_explainer.pred_probs,\n","        'influential_words': word_attributions\n","    }"],"metadata":{"id":"vzH8nG_hqfuk","executionInfo":{"status":"ok","timestamp":1683412958197,"user_tz":240,"elapsed":321,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["index = 8\n","text = test_df.loc[index, 'raw_text'], test_df.loc[index, 'label']\n","text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9vJv9NjUyl6C","executionInfo":{"status":"ok","timestamp":1683412959166,"user_tz":240,"elapsed":6,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}},"outputId":"9c3eba69-bd87-4b2a-d73c-dbded88a30bb"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('O lucro operacional, excluindo itens não recorrentes, totalizou EUR 1,0 mn, abaixo dos EUR 1,6 mn.',\n"," 0)"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["%timeit classify_text(text[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7_w0MV_yaxP","executionInfo":{"status":"ok","timestamp":1683413061073,"user_tz":240,"elapsed":101119,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}},"outputId":"0ce6c089-2e73-4222-ac84-2de787df578e"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["12.5 s ± 629 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}]},{"cell_type":"code","source":["classify_text(text[0])"],"metadata":{"id":"DFhoyN-2BvLz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683413073078,"user_tz":240,"elapsed":12051,"user":{"displayName":"Tiago Santi","userId":"02979615434612127801"}},"outputId":"37abf40b-5c1d-4c71-aed8-498dd46c69ae"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'predicition_class_name': 'LABEL_0',\n"," 'prediction_index': array(0),\n"," 'prediction_probatility': tensor(0.7438),\n"," 'influential_words': [('abaixo', 72.11),\n","  ('totalizou', 6.62),\n","  ('EUR', 5.61),\n","  ('EUR', 3.87),\n","  ('mn', 3.07),\n","  ('itens', 2.71),\n","  ('lucro', 1.74),\n","  ('0', 1.01),\n","  ('1', 0.8),\n","  ('1', 0.78),\n","  ('mn', 0.58),\n","  ('excluindo', 0.45),\n","  ('6', 0.27),\n","  ('recorrentes', 0.19),\n","  ('operacional', 0.19)]}"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":[],"metadata":{"id":"mcDxE6giOLmI"},"execution_count":null,"outputs":[]}]}